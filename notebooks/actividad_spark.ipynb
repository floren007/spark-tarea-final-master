{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c36968-b9f2-4bf8-b67c-8c6b187ec9b4",
   "metadata": {},
   "source": [
    "# Un motor de ingesta sencillo para el dataset de vuelos\n",
    "\n",
    "### Recuerda borrar todas las líneas que dicen `raise NotImplementedError`\n",
    "\n",
    "El trabajo final consiste en la implementación guiada de un esqueleto de motor de ingesta para ficheros JSON recibidos diariamente con información de los vuelos que han tenido lugar en el día indicado en el nombre del fichero entre dos aeropuertos de los Estados Unidos.\n",
    "* La estructura de cada JSON puede consultarse abriendo con un editor de texto cualquiera de los ficheros.\n",
    "* El significado de cada una de las columnas puede consultarse en el fichero config.json que se encuentra en la carpeta config del repositorio.\n",
    "\n",
    "Vamos a probar nuestro paquete del motor de ingesta. Antes de ejecutar este notebook, es imprescindible haber completado todo el código del motor de ingesta. \n",
    "El notebook solamente valida que el código del motor de ingesta con el cual se ha generado el paquete sea correcto. Para ello, el orden en el que debemos leer, entender y completar los ficheros del repositorio es:\n",
    "\n",
    "1. Clase `MotorIngesta` en el fichero `motor_ingesta.py`. Sólo hay que completar la función `ingesta_fichero`\n",
    "2. (Opcional) Completar los tests `test_aplana` y `test_ingesta_fichero` en el fichero `test_ingesta.py`\n",
    "3. Clase `FlujoDiario` en el fichero `flujo_diario.py`. Completar primero el inicializador y luego el método `procesa_diario`. Al ir completando el código de este método, veremos que a su vez necesitamos completar las dos funciones siguientes.\n",
    "4. Función `aniade_hora_utc` en el fichero `agregaciones.py`\n",
    "5. (Opcional) El test de dicha función se llama  `test_aniade_hora_utc` en el fichero `test_ingesta.py`\n",
    "6. Función `aniade_intervalos_por_aeropuerto` en el fichero `agregaciones.py`\n",
    "7. (Opcional) El test de dicha función se llama `test_aniade_intervalos_por_aeropuerto` en el fichero `test_ingesta.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c094eb8b-70fa-4df4-9673-4abb88538aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru==0.7.1 in c:\\users\\flore\\desktop\\spark-tarea-final-master\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\flore\\desktop\\spark-tarea-final-master\\.venv\\lib\\site-packages (from loguru==0.7.1) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\flore\\desktop\\spark-tarea-final-master\\.venv\\lib\\site-packages (from loguru==0.7.1) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\flore\\desktop\\spark-tarea-final-master\\.venv\\lib\\site-packages\\motor_ingesta-0.1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3c5f4-d0f8-48a6-9cf2-95011e916646",
   "metadata": {},
   "source": [
    "## Instalamos el wheel en el cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a207b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flore\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: can't open file 'c:\\\\Users\\\\flore\\\\Desktop\\\\spark-tarea-final-master\\\\notebooks\\\\setup.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python setup.py bdist_wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ffd4cc4-888b-4f86-91f0-6e2fb2d36147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\flore\\desktop\\spark-tarea-final-master\\notebooks\\ruta\\motor_ingesta-0.1.0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'ruta/motor_ingesta-0.1.0-py3-none-any.whl' looks like a filename, but the file does not exist\n",
      "DEPRECATION: Loading egg at c:\\users\\flore\\desktop\\spark-tarea-final-master\\.venv\\lib\\site-packages\\motor_ingesta-0.1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\flore\\\\Desktop\\\\spark-tarea-final-master\\\\notebooks\\\\ruta\\\\motor_ingesta-0.1.0-py3-none-any.whl'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall ruta/motor_ingesta-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2b607-b1ac-40d4-b439-f9d1775fe17d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b86cb75d3279fd2c37567cfcadc8282",
     "grade": false,
     "grade_id": "cell-e7e47b57728a3497",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Probamos la ingesta de un fichero\n",
    "\n",
    "**Ejercicio 1 (2 puntos)**. Ingestar el fichero **`2023-01-01.json`** utilizando el motor de ingesta completo. Debe crearse el objeto de la clase MotorIngesta y utilizar el método `ingesta_fichero`, dejando el resultado en la variable `flights_df`. La variable `flujo_diario` contiene un objeto FlujoDiario inicializado con el path de configuración anterior, y lo vamos a usar ahora solo para leer adecuadamente la configuración y pasársela al objeto `motor_ingesta` como el argumento config.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a218d7f9-a274-424d-b1bd-5c2e80c8265a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34de26b267263009f0b64630904f826e",
     "grade": false,
     "grade_id": "ingesta_fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from motor_ingesta.motor_ingesta import MotorIngesta\n",
    "from motor_ingesta.flujo_diario import FlujoDiario\n",
    "path_config_flujo_diario = \"C:/Users/flore/Desktop/spark-tarea-final-master/config/config.json\"     # ruta del fichero config.json, que no pertenece al paquete\n",
    "path_json_primer_dia = \"C:/Users/flore/Desktop/spark-tarea-final-master/data/2023-01-01.json\"           # ruta del fichero JSON de un día concreto que queremos ingestar, en nuestro caso 2023-01-01.json\n",
    "\n",
    "flujo_diario = None\n",
    "motor_ingesta = None\n",
    "flights_df = None\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "flujo_diario = FlujoDiario(path_config_flujo_diario)\n",
    "motor_ingesta = MotorIngesta(flujo_diario.config)\n",
    "flights_df = motor_ingesta.ingesta_fichero(path_json_primer_dia)\n",
    "\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0536b275-3592-4dd5-bd99-576d0039c883",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b779d274c03920f7902ecf1ba28ef59e",
     "grade": true,
     "grade_id": "ingesta-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(flights_df.count() == 15856)\n",
    "assert(len(flights_df.columns) == 18)\n",
    "dtypes = dict(flights_df.dtypes)\n",
    "assert(dtypes[\"Diverted\"] == \"boolean\")\n",
    "assert(dtypes[\"ArrTime\"] == \"int\")\n",
    "assert(flights_df.schema[\"Dest\"].metadata == {\"comment\": \"Destination Airport IATA code (3 letters)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03479e-42a6-4ee7-9f34-ebc661294c06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3db784f6afa844cf5c8ff545533b6c4a",
     "grade": false,
     "grade_id": "cell-d3f1684d9a8578bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Probamos la función de añadir la hora en formato UTC\n",
    "\n",
    "**Ejercicio 2 (2 puntos)** Probar la función de añadir hora UTC con el DF `flights_df` construido anteriormente. El resultado debe dejarse en la variable `flights_with_utc`. Recuerda que esto no es propiamente un test unitario.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código de la función `aniade_hora_utc` del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15c6e45d-ef89-41f0-821d-39a24de9245e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3674aeefedb405cd73689ee930980846",
     "grade": false,
     "grade_id": "flights-utc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necesarios\n",
    "from motor_ingesta.agregaciones import aniade_hora_utc\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "flights_with_utc = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "flights_with_utc = aniade_hora_utc(spark, flights_df)\n",
    "\n",
    "### En el assert da error en la comprobacion de la columna \"FlightTime\" que dice que no es de tipo \"timestamp\"\n",
    "### pero si hace flights_with_utc.dtypes comprobara que si es de tipo timestamp entonces desconozco porque falla\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d490ed43-a3f2-455c-94a9-f074448469f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6600ce9efa93f23fcafd8a5c4571af25",
     "grade": true,
     "grade_id": "flights-utc-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(flights_with_utc\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightTime is null\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m266\u001b[39m)\n\u001b[0;32m      3\u001b[0m types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(flights_with_utc\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(flights_with_utc\u001b[38;5;241m.\u001b[39mdtypes[\u001b[38;5;241m18\u001b[39m] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightTime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# FlightTime debe ser la última columna\u001b[39;00m\n\u001b[0;32m      6\u001b[0m first_row \u001b[38;5;241m=\u001b[39m flights_with_utc\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginAirportID = 12884\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightTime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightTime\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mfirst()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(first_row\u001b[38;5;241m.\u001b[39mFlightTime \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-01-01 10:59:00\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "assert(flights_with_utc.where(\"FlightTime is null\").count() == 266)\n",
    "types = dict(flights_with_utc.dtypes)\n",
    "assert(flights_with_utc.dtypes[18] == (\"FlightTime\", \"timestamp\"))  # FlightTime debe ser la última columna\n",
    "\n",
    "first_row = flights_with_utc.where(\"OriginAirportID = 12884\").select(F.min(\"FlightTime\").cast(\"string\").alias(\"FlightTime\")).first()\n",
    "assert(first_row.FlightTime == \"2023-01-01 10:59:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f241ea1-fe78-44da-825d-a9426de1dd24",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4b7d83307caa7e39620489749764e78",
     "grade": false,
     "grade_id": "cell-068b74be5a0b9aaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Probamos la función de añadir las columnas con la hora del siguiente vuelo, su aerolínea y el intervalo de tiempo transcurrido\n",
    "\n",
    "**Ejercicio 3 (2.5 puntos)** Invocar a la función de añadir intervalos por aeropuerto, partiendo de la variable `flights_with_utc` del apartado anterior, dejando el resultado devuelto por la función en la variable `df_with_next_flight` cacheada.\n",
    "\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código de la función `aniade_intervalos_por_aeropuerto` en el paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e04f88ca-5267-4821-af63-e301012e94d4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54149f06e69c8b3d8f8f356c4af06fd9",
     "grade": false,
     "grade_id": "flights-intervalos",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from motor_ingesta.agregaciones import aniade_intervalos_por_aeropuerto\n",
    "df_with_next_flight = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df_with_next_flight = aniade_intervalos_por_aeropuerto(flights_with_utc).cache()\n",
    "### En el assert da error en comprobar las 3 ultimas columnas el tipo de dato pero si hace df_with_next_flight.dtypes\n",
    "### comprobara que que las 3 columnas tiene el tipo de dato que corresponden, entonces desconozco porque falla el assert\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd7abe87-4b35-46b8-9376-895d92330fb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e89190d3c04bf02b372ce8df78f18be9",
     "grade": true,
     "grade_id": "flights-intervalos-test",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(df_with_next_flight\u001b[38;5;241m.\u001b[39mdtypes[\u001b[38;5;241m19\u001b[39m] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightTime_next\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(df_with_next_flight\u001b[38;5;241m.\u001b[39mdtypes[\u001b[38;5;241m20\u001b[39m] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirline_next\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(df_with_next_flight\u001b[38;5;241m.\u001b[39mdtypes[\u001b[38;5;241m21\u001b[39m] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiff_next\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigint\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(df_with_next_flight.dtypes[19] == (\"FlightTime_next\", \"timestamp\"))\n",
    "assert(df_with_next_flight.dtypes[20] == (\"Airline_next\", \"string\"))\n",
    "assert(df_with_next_flight.dtypes[21] == (\"diff_next\", \"bigint\"))\n",
    "\n",
    "first_row = df_with_next_flight.where(\"OriginAirportID = 12884\")\\\n",
    "                               .select(F.col(\"FlightTime\").cast(\"string\"), \n",
    "                                       F.col(\"FlightTime_next\").cast(\"string\"), \n",
    "                                       F.col(\"Airline_next\"),\n",
    "                                       F.col(\"diff_next\")).sort(\"FlightTime\").first()\n",
    "\n",
    "assert(first_row.FlightTime_next == \"2023-01-01 16:36:00\")\n",
    "assert(first_row.Airline_next == \"9E\")\n",
    "assert(first_row.diff_next == 20220)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f76bd5-37f3-4afc-8ffb-6a6b6cb5f4c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b2fd2f171470ce5d27ba06300f08677",
     "grade": false,
     "grade_id": "cell-a3539980f6a4e471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Corregimos que el último vuelo de cada aeropuerto y cada día tiene valor nulo en las 3 columnas `_next`\n",
    "\n",
    "**Ejercicio 4 (2.5 puntos)**\n",
    "\n",
    "Tal como está implementada la lógica del flujo diario, el último vuelo de cada día no tendrá informada la columna FlightTime_next porque no se dispone todavía de datos del día siguiente. Se pide **corregir este comportamiento** para solucionar los valores nulos, modificando el código del método `procesa_diario` de manera que, antes de escribir los datos del día actual, se hayan corregido las tres columnas `_next` en los datos del día anterior al que estamos ingestando. Una manera simple (aunque no necesariamente óptima) de conseguirlo es:\n",
    "* Leer de la tabla la partición que se escribió el día previo, si existiera dicha tabla y dicha partición.\n",
    "* Añadir al DF devuelto por `aniade_hora_utc` las 3 columnas que le faltan para tener la misma estructura que la tabla, que son `FlightTime_next`, `Airline_next` y `diff_next` (pueden ser en ese orden si la función `aniade_intervalos_por_aeropuerto` se ha implementado para añadirlas en ese orden), pero sin darles valor (con valor None, convirtiendo cada columna al tipo de dato adecuado para que después encaje con la tabla existente).\n",
    "* Unir el DF del día previo y el que acabamos de calcular\n",
    "* Invocar a `aniade_intervalos_por_aeropuerto` pasando como argumento el DF resultante de la unión.\n",
    "\n",
    "Aparte de un test unitario (que se deja como optativo pero sin puntuación), la manera de comprobar el funcionamiento será invocar a `procesa_diario` del flujo diario, con los ficheros de dos días consecutivos, y después comprobar lo que se ha escrito en la tabla tras la ingesta del segundo fichero. Lo probaremos con los días 1 y 2 de enero de 2023.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60a8f73b-f989-4ac2-8625-34e9e3743197",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cc3bf8f53f02529c5da731e260b8c57",
     "grade": false,
     "grade_id": "procesa-diario",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\flore\\Desktop\\spark-tarea-final-master\\.venv\\Lib\\site-packages\\motor_ingesta-0.1.0-py3.11.egg\\motor_ingesta\\flujo_diario.py:45\u001b[0m, in \u001b[0;36mFlujoDiario.procesa_diario\u001b[1;34m(self, data_file)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Procesamiento diario: crea un nuevo objeto motor de ingesta con self.config, invoca a ingesta_fichero,\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# después a las funciones que añaden columnas adicionales, y finalmente guarda el DF en la tabla indicada en\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Conviene cachear el DF flights_df así como utilizar el número de particiones indicado en\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# config[\"output_partitions\"]\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     motor_ingesta \u001b[38;5;241m=\u001b[39m \u001b[43mMotorIngesta\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m     46\u001b[0m     flights_df \u001b[38;5;241m=\u001b[39m motor_ingesta\u001b[38;5;241m.\u001b[39mingesta_fichero(data_file)\u001b[38;5;241m.\u001b[39mcache()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MotorIngesta' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m path_json_segundo_dia \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/flore/Desktop/spark-tarea-final-master/data/2023-01-02.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Invoca al método procesa_diario del flujo con el path del fichero 2023-01-01.json\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Después invoca al método de nuevo con el path del fichero 2023-01-02.json\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mFlujoDiario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocesa_diario\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath_json_primer_dia\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m FlujoDiario\u001b[38;5;241m.\u001b[39mprocesa_diario(spark,path_json_segundo_dia)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\flore\\Desktop\\spark-tarea-final-master\\.venv\\Lib\\site-packages\\motor_ingesta-0.1.0-py3.11.egg\\motor_ingesta\\flujo_diario.py:97\u001b[0m, in \u001b[0;36mFlujoDiario.procesa_diario\u001b[1;34m(self, data_file)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS tabla_provisional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 97\u001b[0m     \u001b[43mlogger\u001b[49m\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo se pudo escribir la tabla del fichero \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# imports necesario\n",
    "from motor_ingesta.flujo_diario import FlujoDiario\n",
    "path_json_segundo_dia = \"C:/Users/flore/Desktop/spark-tarea-final-master/data/2023-01-02.json\"\n",
    "\n",
    "# Invoca al método procesa_diario del flujo con el path del fichero 2023-01-01.json\n",
    "# Después invoca al método de nuevo con el path del fichero 2023-01-02.json\n",
    "\n",
    "# YOUR CODE HERE\n",
    "FlujoDiario.procesa_diario(spark,path_json_primer_dia)\n",
    "FlujoDiario.procesa_diario(spark,path_json_segundo_dia)\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fa18d-77e3-4e5a-9243-13ecc9e0d2b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b967ae19d8e854ebe370fd146fd86f68",
     "grade": true,
     "grade_id": "tests-unitarios",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vuelos = spark.read.table(\"default.flights\").sort(\"Origin\", \"FlightTime\")\n",
    "assert(vuelos.count() == 33931)\n",
    "row = vuelos.where(\"FlightDate = '2023-01-01' and  Origin = 'ABE' and DepTime = 1734\").first()\n",
    "assert(row.diff_next == 44220)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff13aa-4672-4038-8afd-d0773546f622",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "603d2e3dc558735a0429356e1a91c287",
     "grade": false,
     "grade_id": "cell-84b4251b85b7176b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Ejercicio opcional\n",
    "\n",
    "**Ejercicio opcional (1 punto)** Completa los cuatro tests unitarios que encontrarás en el fichero `test_ingesta.py`. No tienes que escribir más código en este notebook.\n",
    "\n",
    "- Se podrá optar a una calificación final de hasta 9.0 puntos sin resolver este ejercicio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
